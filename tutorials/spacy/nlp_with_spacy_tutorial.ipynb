{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [NLP with Spacy](https://conferences.oreilly.com/jupyter/jup-ny/public/schedule/detail/59562)\n",
    "\n",
    "Relevant git repository: https://github.com/datascienceinc/jupytercon-2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization:\n",
    "\n",
    "What is tokenization? Separating out a document / corpus into useful / distinct tokens.  Naive approach: separating text via whitespace. Cons with such approach:\n",
    "* a proper noun such as Great Britain might qualify it's own token\n",
    "* punctuation will get looped into the preceding word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Spacy does it:\n",
    "\n",
    "Previous attempts heavily leaned on regex, but has limits.  Rough psuedocode of SpaCy's tokenizer:\n",
    "\n",
    "(taken from: https://github.com/datascienceinc/jupytercon-2017/blob/master/tutorial/2-Text-Processing-and-Tokenization.ipynb)\n",
    "* Split text by whitespace.\n",
    "* Iterate over space-separated substrings: \n",
    "    * The dog doesn't shop at Macy's (anymore).$\\rightarrow$ [The, dog, doesn't, shop, at, Macy's, (anymore).]\n",
    "* Check whether we have an explicitly defined rule for this substring. If we do, use it.\n",
    "    * doesn't $\\rightarrow$ [does, n't]\n",
    "* Otherwise, try to consume a prefix.\n",
    "* (anymore). $\\rightarrow$ [(, anymore).]\n",
    "* If we consumed a prefix, go back to the beginning of the loop, so that special-cases always get priority.\n",
    "* If we didn't consume a prefix, try to consume a suffix.\n",
    "    * anymore). $\\rightarrow$ [anymore, ), .]\n",
    "* If we can't consume a prefix or suffix, look for \"infixes\" â€” stuff like hyphens etc.\n",
    "* Once we can't consume any more of the string, handle it as a single token.\n",
    "    * [The, dog, does, n't, shop, at, Macy's, (, anymore, ), .]\n",
    "    \n",
    "#### Extending:\n",
    "\n",
    "Can extend in a variety of ways:\n",
    "* adding special tokenization cases\n",
    "* modifying how a tokenizer operates / handles prefixes/suffixes\n",
    "* creating a whole new tokenizer\n",
    "\n",
    "(see bottom of [notebook 2](https://github.com/datascienceinc/jupytercon-2017/blob/master/tutorial/2-Text-Processing-and-Tokenization.ipynb) for examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Word / Document Representations](https://github.com/datascienceinc/jupytercon-2017/blob/master/tutorial/3-Word-Embeddings.ipynb)\n",
    "\n",
    "### Document Representation\n",
    "\n",
    "`sklearn` provides both Count and TFIDF vectorization implementations. Often useful for Doc. Rep (recall 20 Newsgroups assignment in COMP330).  Issue with Count / TFIDF implementations is that they require the whole corpus, rather than being able to add tokens online.\n",
    "\n",
    "#### Online Token Addition: [Feature Hashing for Stateless Transformations](https://arxiv.org/pdf/0902.2206.pdf)\n",
    "\n",
    "Rather than giving each unique token its own spot in the document vector representation, hash the token, mod the hash by the vector's dimensionality and increment that index:\n",
    "* v $\\leftarrow$ vector of length $K$\n",
    "* for each token do\n",
    " - hash $\\leftarrow$ $H$(token)\n",
    " - $v_{hash} \\leftarrow v_{hash} + 1$\n",
    " \n",
    "\n",
    "### Word Representation\n",
    "\n",
    "Simplest representation is simply a term term matrix, i.e. the dot product of the document term matrix (shape: [n sentences, n words]) with itself. Basically, it counts the co-occurances of two words / tokens in the corpus.\n",
    "\n",
    "Often just the co-occurances aren't so useful, one typically uses Positive Pointwise Mutual Information (PPMI):  \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{PMI}(x, y) = log_2\\frac{P(x,y)}{P(x)P(y)}\n",
    "\\end{equation}\n",
    "\n",
    "Recall probabilistic independence:\n",
    "\\begin{equation}\n",
    "P(A, B) = P(A)P(B)\n",
    "\\end{equation}\n",
    "\n",
    "Note that the denominator here divides over the product of the likelihood of seeing $x$ and $y$ in general, rather than together.  So the PMI is computing how likely seeing $x$ and $y$ together over seeing them in general.  (If they are independent we compute $log_2(1) = 0$, if they are 'super dependent' we compute something like $log_2\\left(\\frac{1.0}{(0.1) * (0.2)}\\right) \\approx 7.5$, for example).\n",
    "\n",
    "#### Neural Embeddings\n",
    "\n",
    "Some images providing some intuition:\n",
    "![NN](jupytercon-2017/tutorial/assets/softmax-nplm.png)\n",
    "![NN](jupytercon-2017/tutorial/assets/BengioNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [POS Tagging](https://github.com/datascienceinc/jupytercon-2017/blob/master/tutorial/4-Part-of-Speech-Tagging.ipynb)\n",
    "\n",
    "(Last Assignment in 182)\n",
    "\n",
    "### Overview of what talk covered\n",
    "\n",
    "* What POS are\n",
    "* Why they are useful in natural language processing\n",
    "* Why word vectors need to take into consideration POS\n",
    "* What SpaCy provides, how to extend it / train your own tagger\n",
    "* Full POS tagger from start to finish with SpaCy\n",
    "* A bit about perceptrons / weighted perceptrons\n",
    "\n",
    "In SpaCy's natural language processing unit, they automatically tag the parts of speech of a sentence when consumed:  \n",
    "\n",
    "```python\n",
    "sentence = nlp(\"I went to the store\")\n",
    "sentence[0].tag_\n",
    "```\n",
    "\n",
    "so that the `tag_` attribute is available to the user.  You can also train your own, which requires:\n",
    "\n",
    "* a vocabulary object: stores lexeme data\n",
    "* a statistical model for prediction: consumes features (such as POS of a word, a preceding word, a following word, brown cluster information, etc...)\n",
    "* a tagger: uses the vocab and the model to tag the tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Deep Learning for Classification](https://github.com/datascienceinc/jupytercon-2017/blob/master/tutorial/6-Deep-Learning-for-Classification.ipynb)\n",
    "\n",
    "Briefly discussed the history of language models and what they are.  In general, a language model is:  \n",
    "\n",
    "\\begin{equation}\n",
    "P(word_i | context)\n",
    "\\end{equation}\n",
    "\n",
    "### History (Taken from link above)\n",
    "\n",
    "* Old approach: ngram model: \n",
    "    * empirically estimate $p(word_i | word_{i-1}, word_{i-2})$\n",
    "\n",
    "* Bengio et al 2003: A neural probabilistic language model:\n",
    "    * Too many words! High dimensionality, low generality of ngram models.\n",
    "    * Not leveraging dependencies learned between individual words.\n",
    "    * Ignoring words outside a very small context window.\n",
    "    * Solution: Construct a feed forward network to predict context given word (or visa versa), express word probabilities in terms of the intermediate feature vector representation learned by the network.\n",
    "    * Bonus: Models that assign high probabilities to word sequences \"understand\" them, so the intermediate representations are likely useful for other tasks  (word embeddings).\n",
    "    \n",
    "    ![Kiku](jupytercon-2017/tutorial/assets/BengioNN.png)\n",
    "    \n",
    "* Milokov 2010: \n",
    "    * Feed forward model requires a fixed length window (researcher degree of freedom, etc).\n",
    "        * Long range dependencies are lost!\n",
    "        * Context confined to neighborhoods defined by heuristics.\n",
    "    * Use recurrent connections to pass contextual information through arbitrarily long (theoretically) sequences of words:\n",
    "    \n",
    "    ![RNN](jupytercon-2017/tutorial/assets/RNN.png)\n",
    "    \n",
    "    such that: \n",
    "    * $h_t = \\text{activation}(W_h h_{t-1} + W_x x_t) $\n",
    "    * $y_t = \\text{softmax}(W_y h_t)$\n",
    "    * $W_x$ can be used as an embedding matrix\n",
    "    \n",
    "    Challenges and Extensions:\n",
    "    * Ignores future words $\\rightarrow$ bidirectional rnns\n",
    "    * numerical challenges with gradients (exploding/vanishing)\n",
    "    * hard to train weights that retain long term dependencies $\\rightarrow$ trainable activation units\n",
    "        * gated recurrent units or LSTMs\n",
    "            * train the degree to which we update our hidden state with information from the word vs last hidden state. I.E., should we keep this hidden layer highway going, or should we forget context?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 ML",
   "language": "python",
   "name": "pythonml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
